{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002). \n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**. \n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "SaZnxptMJiD7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.6\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download). \n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects. \n",
        " \n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "# from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Generator, MutableSet\n",
        "from functools import reduce\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-rakqtlMOT"
      },
      "outputs": [],
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "careEKj-mRpl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences, \n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "  \n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4: \n",
        "    break\n",
        "    \n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "      \n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "# (1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ogq0Eq2hQglh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "def read_lexicon(filename: str) -> Tuple[List[List[str]], Dict, Dict]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            filename: the file path of the lexicon\n",
        "        Returns:\n",
        "            List of shape Nx3, where N is the length of the lexicon and each entry has the following attributes (in this order):\n",
        "                - word\n",
        "                - sentiment\n",
        "                - magnitude\n",
        "            Dictionary that maps words from the lexicon to the corresponding sentiments\n",
        "            Dictionary that maps words from the lexicon to the corresponding sentiment magnitudes\n",
        "    \"\"\"\n",
        "    def map_polarity_to_integer(polarity: str) -> int:  \n",
        "        if polarity == \"negative\":\n",
        "            return -1\n",
        "        elif polarity == \"positive\":\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    with open(filename, mode=\"r\", encoding=\"utf-8\") as r:\n",
        "        lexicon = r.readlines()\n",
        "        \n",
        "        # remove endline for each line in the lexicon file\n",
        "        lexicon = [entry.strip(\"\\n\") for entry in lexicon] \n",
        "\n",
        "        # get every attribute individually\n",
        "        lexicon = [entry.split(\" \") for entry in lexicon]\n",
        "\n",
        "        # get onyl word, sentiment, and magnitude (remove subj for magnitude)\n",
        "        lexicon = [[entry[2], entry[-1], entry[0][ : -4]] for entry in lexicon]\n",
        "\n",
        "        # remove the attrbute marker\n",
        "        lexicon = [[attribute.split(\"=\")[1] for attribute in line] for line in lexicon]\n",
        "\n",
        "    # compute sentiments dictionary\n",
        "    # assign 1 to a positive sentiment and -1 to a negative one \n",
        "    sentiments = {entry[0] : map_polarity_to_integer(entry[1]) for entry in lexicon}\n",
        "\n",
        "    # compute magnitudes dictionary\n",
        "    # assign 1 to a weak sentiment and 2 to a strong one\n",
        "    magnitudes = {entry[0] : (1 if entry[2] == \"weak\" else 2) for entry in lexicon}\n",
        "\n",
        "    return lexicon, sentiments, magnitudes\n",
        "\n",
        "def read_dataset(filename: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            filename: the file path of the dataset\n",
        "        Returns:\n",
        "            3 numpy arrays:\n",
        "                1) the sentiment of each document\n",
        "                2) the documents in the order they appear in filename (only the words without the pos tags)\n",
        "                3) the documents in the order they appear in filename (only the pos tags without the words)\n",
        "    \"\"\"\n",
        "    with open(filename, mode=\"r\", encoding=\"utf-8\") as r:\n",
        "        reviews = json.load(r)\n",
        "\n",
        "    sentiments = []\n",
        "    documents = []\n",
        "    documents_pos_tags = []\n",
        "\n",
        "    for review in reviews:\n",
        "        document = []\n",
        "        document_pos_tags = [] \n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            words = []\n",
        "            pos_tags = []\n",
        "\n",
        "            # separate the words and pos tags\n",
        "            for word, pos_tag in sentence:\n",
        "                words.append(word.lower()) # remove casing for the words in the documents\n",
        "                pos_tags.append(pos_tag)\n",
        "\n",
        "            # rebuild the sentence\n",
        "            document.append(words)\n",
        "            document_pos_tags.append(pos_tags)\n",
        "\n",
        "        sentiments.append(1 if review[\"sentiment\"] == \"POS\" else 0)\n",
        "        documents.append(document)\n",
        "        documents_pos_tags.append(document_pos_tags)\n",
        "\n",
        "    sentiments = np.array(sentiments)\n",
        "    documents = np.array(documents, dtype=object)\n",
        "    documents_pos_tags = np.array(documents_pos_tags, dtype=object)\n",
        "\n",
        "    return sentiments, documents, documents_pos_tags\n",
        "\n",
        "lexicon, lexicon_sentiments, lexicon_magnitudes = read_lexicon(\"sent_lexicon\")\n",
        "documents_sentiments, documents, documents_pos_tags = read_dataset(\"reviews.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy528EUTphz5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.677\n"
          ]
        }
      ],
      "source": [
        "def basic_binary_lexicon_classification(documents: np.ndarray, documents_sentiments: np.ndarray, lexicon_sentiments: Dict, threshold: int = 8) -> Tuple[List[int], float]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: an array containing all the documents to be processed\n",
        "            documents_sentiments: the respective sentiments of each document\n",
        "            lexicon_sentiments: a dictionary that maps words to their corresponding prior sentiments\n",
        "            threshold: the threshold used for classification\n",
        "        Returns:\n",
        "            1) a list of 1s and 0s, where 1 means that the classification made by the lexicon-based approach is correct and 0 otherwise\n",
        "            2) the lexicon-based classification accuracy\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    ground_truth = documents_sentiments\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "        for sentence in document:\n",
        "            for word in sentence:\n",
        "                if word in lexicon_sentiments.keys(): # add 0 to the score for words that do not exist in the lexicon\n",
        "                    score += lexicon_sentiments[word]\n",
        "\n",
        "        if score > threshold:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    \n",
        "    accuracy = sk.metrics.accuracy_score(ground_truth, predictions)\n",
        "\n",
        "    return (predictions == ground_truth), accuracy\n",
        "\n",
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "token_results, token_accuracy = basic_binary_lexicon_classification(documents, documents_sentiments, lexicon_sentiments)\n",
        "print(\"Accuracy: %0.3f\" % token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The average score across the documents is: 10.8195\n"
          ]
        }
      ],
      "source": [
        "def compute_average_score_lexicon_magnitude_approach(documents: np.ndarray, lexicon_sentiments: Dict, lexicon_magnitudes: Dict) -> float:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: an array containing all the documents to be processed\n",
        "            lexicon_sentiments: a dictionary that maps words to their corresponding prior sentiments\n",
        "            lexicon_magnitudes: a dictionary that maps words to their corresponding sentiment magnitudes\n",
        "        Returns:\n",
        "            the average score across the documents\n",
        "    \"\"\"\n",
        "    total_score = 0\n",
        "\n",
        "    for document in documents:\n",
        "        for sentence in document:\n",
        "            for word in sentence:\n",
        "                if word in lexicon_sentiments.keys(): # add 0 to the score for words that do not exist in the lexicon\n",
        "                    total_score += lexicon_magnitudes[word] * lexicon_sentiments[word]\n",
        "    \n",
        "    return total_score / documents.shape[0]\n",
        "\n",
        "def magnitude_binary_lexicon_classification(documents: np.ndarray, documents_sentiments: np.ndarray, lexicon_sentiments: Dict, lexicon_magnitudes: Dict) -> Tuple[List[int], float]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: an array containing all the documents to be processed\n",
        "            documents_sentiments: the respective sentiments of each document\n",
        "            lexicon_sentiments: a dictionary that maps words to their corresponding prior sentiments\n",
        "            lexicon_magnitudes: a dictionary that maps words to their corresponding sentiment magnitudes\n",
        "        Returns:\n",
        "            1) a list of 1s and 0s, where 1 means that the classification made by the lexicon-based approach is correct and 0 otherwise\n",
        "            2) the lexicon-based classification accuracy\n",
        "    \"\"\"\n",
        "    THRESHOLD = 11\n",
        "\n",
        "    predictions = []\n",
        "    ground_truth = documents_sentiments\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "        for sentence in document:\n",
        "            for word in sentence:\n",
        "                if word in lexicon_sentiments.keys(): # add 0 to the score for words that do not exist in the lexicon\n",
        "                    score += lexicon_magnitudes[word] * lexicon_sentiments[word]\n",
        "\n",
        "        if score > THRESHOLD:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    \n",
        "    accuracy = sk.metrics.accuracy_score(ground_truth, predictions)\n",
        "\n",
        "    return (predictions == ground_truth), accuracy\n",
        "\n",
        "print(f\"The average score across the documents is: {compute_average_score_lexicon_magnitude_approach(documents, lexicon_sentiments, lexicon_magnitudes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9vVk7CvDpyka"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.696\n"
          ]
        }
      ],
      "source": [
        "magnitude_results, magnitude_accuracy = magnitude_binary_lexicon_classification(documents, documents_sentiments, lexicon_sentiments, lexicon_magnitudes)\n",
        "print(\"Accuracy: %0.3f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "8LgBcYcXsEk3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 2 artists>"
            ]
          },
          "execution_count": 220,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar([\"without magnitude\", \"with magnitude\"], [token_accuracy, magnitude_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "We could receive a short document only containing a few positive words.\n",
        "In this case it's likely that, although only received positive words, the document is still classified as negative solely because the binary score didn't amount to the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dwt0B8h8aKjr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New threshold: 0.22473561523462227\n",
            "Accuracy: 0.634\n"
          ]
        }
      ],
      "source": [
        "def compute_threshold_lexicon_doc_length(documents: np.ndarray, lexicon_sentiments: Dict) -> float:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: an array containing all the documents to be processed\n",
        "            lexicon_sentiments: a dictionary that maps words to their corresponding prior sentiments\n",
        "        Returns:\n",
        "            the average score across the documents\n",
        "    \"\"\"\n",
        "    total_score = 0\n",
        "\n",
        "    for document in documents:\n",
        "        document_score = 0\n",
        "        for sentence in document:\n",
        "            for word in sentence:\n",
        "                if word in lexicon_sentiments.keys(): # add 0 to the score for words that do not exist in the lexicon\n",
        "                    document_score += lexicon_sentiments[word]\n",
        "        total_score += document_score / len(document)\n",
        "    \n",
        "    return total_score / documents.shape[0]\n",
        "\n",
        "print(f\"New threshold: {compute_threshold_lexicon_doc_length(documents, lexicon_sentiments)}\")\n",
        "results, accuracy = basic_binary_lexicon_classification(documents, documents_sentiments, lexicon_sentiments, threshold=1)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**.  What would be the problem instead with skipping words only for one class in case 2? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "Having information of words only for one sentiment class means having an imbalanced dataset which we fit our model on. Strongly positive words are expected to be seen more often in positive reviews. However, we do expected them to also occur ironically or with a negation in front in negative reviews.\n",
        "Skipping these words for both classes is a way to 'balance' the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G7zaJYGFvIJ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.825\n"
          ]
        }
      ],
      "source": [
        "def get_documents_bows(documents: List, documents_sentiments: List) -> List:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            documents_sentiments: their respective sentiment labels\n",
        "        Returns:\n",
        "            1) list that contains the bows for each class\n",
        "    \"\"\"\n",
        "    labels = np.unique(documents_sentiments)\n",
        "    \n",
        "    bows = []\n",
        "\n",
        "    for label in labels:\n",
        "        # get only the documents relevant for the current label\n",
        "        current_documents = documents[np.squeeze(np.argwhere(documents_sentiments == label))] \n",
        "\n",
        "        # flatten the documents\n",
        "        concatenated_documents = np.array([])\n",
        "\n",
        "        for document in current_documents:\n",
        "            flat_document = np.array(reduce(lambda x, y: x + y, document))\n",
        "            concatenated_documents = np.concatenate((concatenated_documents, flat_document))\n",
        "\n",
        "        words, counts = np.unique(concatenated_documents, return_counts=True)\n",
        "\n",
        "        bow = {}\n",
        "        for word, count in zip(words, counts):\n",
        "            bow[word] = count\n",
        "\n",
        "        bows.append(bow)\n",
        "        \n",
        "    return bows\n",
        "\n",
        "def train_naive_bayes(documents: List, documents_sentiments: List, bows: List, k: int = 0) -> Tuple[List, List, MutableSet]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            documents_sentiments: their respective sentiment labels\n",
        "            bows: the bows for each class\n",
        "            k: the smoothing value\n",
        "        Returns:\n",
        "            1) list that contains the priors of each class\n",
        "            2) list of dictionaries that map words to their respective conditional probabilities, for each class\n",
        "            3) the vocabulary containing all the words found in the documents\n",
        "    \"\"\"\n",
        "    vocabulary = set()\n",
        "    for document in documents:\n",
        "        document_vocabulary = set(reduce(lambda x, y: x + y, document))\n",
        "        vocabulary = vocabulary.union(document_vocabulary)\n",
        "\n",
        "    labels = np.unique(documents_sentiments)\n",
        "\n",
        "    priors = []\n",
        "    conditionals = []\n",
        "\n",
        "    for label in labels:\n",
        "        # get only the documents relevant for the current label\n",
        "        current_documents = documents[np.squeeze(np.argwhere(documents_sentiments == label))] \n",
        "\n",
        "        prior = len(current_documents) / len(documents)\n",
        "        priors.append(np.log(prior))\n",
        "\n",
        "        conditional = {}\n",
        "\n",
        "        total_vocabulary_occurances = 0\n",
        "        for word in vocabulary:\n",
        "            if word in bows[label]:\n",
        "                total_vocabulary_occurances += bows[label][word]\n",
        "\n",
        "        for word in vocabulary:\n",
        "            if word in bows[label]:\n",
        "                conditional[word] = np.log((bows[label][word] + k) / (total_vocabulary_occurances + k * len(vocabulary)))\n",
        "\n",
        "        conditionals.append(conditional)\n",
        "\n",
        "    return priors, conditionals, vocabulary\n",
        "\n",
        "def classify_naive_bayes(documents: List, priors: List, conditionals: Dict) -> List:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            priors: list of priors for each class\n",
        "            conditionals: list of dictionaries that map words to their respective conditional probabilities, for each class\n",
        "        Returns:\n",
        "            a list of predictions for each document\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for document in documents:\n",
        "        score_0 = priors[0]\n",
        "        score_1 = priors[1]\n",
        "        for sentence in document:\n",
        "            for word in sentence:\n",
        "                if word in conditionals[0].keys() and word in conditionals[1].keys():\n",
        "                    score_0 += conditionals[0][word]\n",
        "                    score_1 += conditionals[1][word]\n",
        "        \n",
        "        if score_0 >= score_1:\n",
        "            predictions.append(0)\n",
        "        else:\n",
        "            predictions.append(1)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def split_dataset(documents: List, documents_sentiments: List, negative_training_indices: np.ndarray, negative_testing_indices: np.ndarray, positive_training_indices: np.ndarray, positive_testing_indices: np.ndarray) -> Tuple[List, List, List, List]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be split\n",
        "            documents_sentiments: the corresponding sentiment labels\n",
        "            negative_indices: a list of indices that specify which negative documents should be used for training\n",
        "            positive_indices: a list of indices that specify which positive documents should be used for training\n",
        "            negative_training_indices: a list of indices that specify which negative documents should be used for training\n",
        "            negative_testing_indices: a list of indices that specify which negative documents should be used for testing\n",
        "            positive_training_indices: a list of indices that specify which positive documents should be used for training\n",
        "            positive_testing_indices: a list of indices that specify which positive documents should be used for testing\n",
        "        Returns:\n",
        "            The trainig/testing split\n",
        "    \"\"\"\n",
        "    negative_documents = documents[ : 1000]\n",
        "    negative_labels = documents_sentiments[ : 1000]\n",
        "    positive_documents = documents[1000 : ]\n",
        "    positive_labels = documents_sentiments[1000 : ]\n",
        "\n",
        "    negative_training_documents = negative_documents[negative_training_indices]\n",
        "    negative_training_labels = negative_labels[negative_training_indices]\n",
        "    negative_testing_documents = negative_documents[negative_testing_indices]\n",
        "    negative_testing_labels = negative_labels[negative_testing_indices]\n",
        "\n",
        "    positive_training_documents = positive_documents[positive_training_indices]\n",
        "    positive_training_labels = positive_labels[positive_training_indices]\n",
        "    positive_testing_documents = positive_documents[positive_testing_indices]\n",
        "    positive_testing_labels = positive_labels[positive_testing_indices]\n",
        "\n",
        "    training_documents = np.concatenate((negative_training_documents, positive_training_documents))\n",
        "    testing_documents = np.concatenate((negative_testing_documents, positive_testing_documents))\n",
        "    training_labels = np.concatenate((negative_training_labels, positive_training_labels))\n",
        "    testing_labels = np.concatenate((negative_testing_labels, positive_testing_labels))\n",
        "\n",
        "    return training_documents, testing_documents, training_labels, testing_labels\n",
        "\n",
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows)\n",
        "predictions = classify_naive_bayes(testing_documents, priors, conditionals)\n",
        "\n",
        "accuracy = sk.metrics.accuracy_score(predictions, testing_labels)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "Accuracy is not a good metric in this situation because a classifier that predicts only positive sentiments is going to have 90% accuracy, but we know that only predicting positive sentiments is not a good approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "GWDkt5ZrrFGp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.600\n"
          ]
        }
      ],
      "source": [
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(90)), np.array(range(900, 910)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows)\n",
        "predictions = classify_naive_bayes(testing_documents, priors, conditionals)\n",
        "\n",
        "accuracy = sk.metrics.accuracy_score(predictions, testing_labels)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$ for a word\n",
        "$w_i$ becomes\n",
        "$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the impact on performance. \n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "g03yflCc9kpW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.835\n"
          ]
        }
      ],
      "source": [
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "predictions = classify_naive_bayes(testing_documents, priors, conditionals)\n",
        "\n",
        "accuracy = sk.metrics.accuracy_score(predictions, testing_labels)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "3KeCGPa7Nuzx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.785, 0.85, 0.81, 0.87, 0.8, 0.865, 0.825, 0.79, 0.825, 0.82] 0.8240000000000001\n"
          ]
        }
      ],
      "source": [
        "def round_robin_k_fold(no_samples: int, k: int = 10) -> Generator[Tuple[List, List], None, None]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            no_samples: the number of input samples\n",
        "            k: the number of folds\n",
        "        Returns:\n",
        "            generator that yields two lists, the training and testing folds for each iteration\n",
        "    \"\"\"\n",
        "    folds = []\n",
        "\n",
        "    for i in range(k):\n",
        "        fold = [10 * j + i for j in range(no_samples // k)]\n",
        "        folds.append(fold)\n",
        "\n",
        "    for i, testing_fold in enumerate(folds):\n",
        "        training_fold = folds[ : i] + folds[i + 1 : ]\n",
        "        training_fold = reduce(lambda x, y: x + y, training_fold)\n",
        "\n",
        "        yield training_fold, testing_fold\n",
        "\n",
        "def evaluate_naive_bayes(documents: List, documents_sentiments: List, smoothing: int = 0) -> Tuple[List, float]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            documents_sentiments: the respective sentiment labels\n",
        "            smoothing: the smoothing factor to be used in Naive Bayes\n",
        "        Returns:\n",
        "            the list of accuracies for each fold and the mean accuracy across all folds\n",
        "    \"\"\"\n",
        "    accuracies = []\n",
        "\n",
        "    for training_fold, testing_fold in round_robin_k_fold(len(documents)):\n",
        "        training_documents = documents[training_fold]\n",
        "        testing_documents = documents[testing_fold]\n",
        "        training_labels = documents_sentiments[training_fold]\n",
        "        testing_labels = documents_sentiments[testing_fold]\n",
        "\n",
        "        bows = get_documents_bows(training_documents, training_labels)\n",
        "\n",
        "        priors, conditionals, _ = train_naive_bayes(training_documents, training_labels, bows, k=smoothing)\n",
        "        predictions = classify_naive_bayes(testing_documents, priors, conditionals)\n",
        "\n",
        "        accuracy = sk.metrics.accuracy_score(predictions, testing_labels)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    return accuracies, sum(accuracies) / len(accuracies)\n",
        "    \n",
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "accuracies, accuracy = evaluate_naive_bayes(documents, documents_sentiments, smoothing=1)\n",
        "\n",
        "print(accuracies, accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "ZoBQm1KuNzNR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance: 0.00079\n"
          ]
        }
      ],
      "source": [
        "def compute_variance(x: List) -> float:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            x: the list of values the variance has to be computed for\n",
        "        Returns:\n",
        "            the variance \n",
        "    \"\"\"\n",
        "    mean = sum(x) / len(x)\n",
        "    values = [(value - mean) ** 2 for value in x]\n",
        "\n",
        "    return sum(values) / len(values)\n",
        "\n",
        "print(\"Variance: %0.5f\" % compute_variance(accuracies))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "def apply_stemming(documents: List) -> List:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "        Returns:\n",
        "            The documents after applying Porter stemming on the words\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        new_sentences = []\n",
        "\n",
        "        for sentence in document:\n",
        "            new_words = []\n",
        "\n",
        "            for word in sentence:\n",
        "                new_word = stemmer.stem(word) # stemming\n",
        "                new_words.append(new_word)\n",
        "\n",
        "            new_sentences.append(new_words)\n",
        "\n",
        "        new_documents.append(new_sentences)\n",
        "    \n",
        "    return np.array(new_documents, dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "gYqKBOiIrInT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.818\n"
          ]
        }
      ],
      "source": [
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "documents = apply_stemming(documents)\n",
        "_, accuracy = evaluate_naive_bayes(documents, documents_sentiments, smoothing=1)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "MA3vee5-rJyy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the vocabulary without stemming: 45348\n",
            "The size of the vocabulary with stemming: 32404\n"
          ]
        }
      ],
      "source": [
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "\n",
        "print(f\"The size of the vocabulary without stemming: {len(vocabulary)}\")\n",
        "\n",
        "documents = apply_stemming(documents)\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "\n",
        "print(f\"The size of the vocabulary with stemming: {len(vocabulary)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.829 using up to 2-grams\n",
            "Accuracy 0.8145 using up to 3-grams\n"
          ]
        }
      ],
      "source": [
        "def compute_ngrams(documents: List, n: int) -> List:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed i.e. each sentence is going to be replaced by a list of ngrams (unigrams+bigrams+...+ngrams)\n",
        "            n: the maximum ngram length\n",
        "        Returns:\n",
        "            the new documents with the sentences replaced by lists of ngrams\n",
        "    \"\"\"\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        new_document = []\n",
        "        for sentence in document:\n",
        "            new_sentence = []\n",
        "            for i in range(1, n + 1):\n",
        "                new_sentence += ngrams(sentence, i)\n",
        "            \n",
        "            new_document.append(new_sentence)\n",
        "\n",
        "        new_documents.append(new_document)\n",
        "\n",
        "    return np.array(new_documents, dtype=object)\n",
        "\n",
        "for i in range(2, 4):\n",
        "    documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "    documents = compute_ngrams(documents, i)\n",
        "\n",
        "    _, accuracy = evaluate_naive_bayes(documents, documents_sentiments, smoothing=1)\n",
        "    print(f\"Accuracy {accuracy} using up to {i}-grams\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "For a sentence of length N, the number of unigrams is N, the number of bigrams is N - 1 and the number of trigrams is N - 2. Considering this, we would expected the number of Bow features to increase linearily. The number of features for the stemming approach should be less than the number of features for the approach without stemming, and in turn much less than the number of features for the ngram approach.\n",
        "\n",
        "The size of the vocabulary without stemming: 45348\n",
        "\n",
        "The size of the vocabulary with stemming: 32404\n",
        "\n",
        "The size of the vocabulary with unigrams: 45348\n",
        "\n",
        "The size of the vocabulary with unigrams+bigrams: 465262\n",
        "\n",
        "The size of the vocabulary with unigrams+bigrams+trigrams: 1346107"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "_z8sAJeUrdtM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the vocabulary with unigrams: 45348\n",
            "The size of the vocabulary with unigrams+bigrams: 465262\n",
            "The size of the vocabulary with unigrams+bigrams+trigrams: 1346107\n"
          ]
        }
      ],
      "source": [
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "\n",
        "print(f\"The size of the vocabulary with unigrams: {len(vocabulary)}\")\n",
        "\n",
        "documents = compute_ngrams(documents, 2)\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "\n",
        "print(f\"The size of the vocabulary with unigrams+bigrams: {len(vocabulary)}\")\n",
        "\n",
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "documents = compute_ngrams(documents, 3)\n",
        "training_documents, testing_documents, training_labels, testing_labels = split_dataset(documents, documents_sentiments, np.array(range(900)), np.array(range(900, 1000)), np.array(range(900)), np.array(range(900, 1000)))\n",
        "bows = get_documents_bows(training_documents, training_labels)\n",
        "priors, conditionals, vocabulary = train_naive_bayes(training_documents, training_labels, bows, k=1)\n",
        "\n",
        "print(f\"The size of the vocabulary with unigrams+bigrams+trigrams: {len(vocabulary)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of \n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM accuracy: 0.839\n",
            "Naive Bayes accuracy: 0.824\n"
          ]
        }
      ],
      "source": [
        "def get_count_vectorizer(training_documents: List) -> sk.feature_extraction.text.CountVectorizer:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            training_documents: the documents used for training the svm model\n",
        "        Returns:\n",
        "            a CountVectorizer that will be used to transform the data at training/inference time\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for document in training_documents:\n",
        "        flat_document = reduce(lambda x, y: x + y, document)\n",
        "        documents.append(\" \".join(flat_document))\n",
        "\n",
        "    count_vectorizer = sk.feature_extraction.text.CountVectorizer(lowercase=False).fit(documents)\n",
        "\n",
        "    return count_vectorizer\n",
        "\n",
        "def svm_process_documents(documents: List, count_vectorizer: sk.feature_extraction.text.CountVectorizer) -> scipy.sparse._csr.csr_matrix:\n",
        "    new_documents = []\n",
        "    for document in documents:\n",
        "        flat_document = reduce(lambda x, y: x + y, document)\n",
        "        new_documents.append(\" \".join(flat_document))\n",
        "\n",
        "    return count_vectorizer.transform(new_documents)\n",
        "\n",
        "def evaluate_svm(documents: List, documents_sentiments: List) -> Tuple[List, float]:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            documents_sentiments: the respective sentiment labels\n",
        "        Returns:\n",
        "            list of accuracies for each fold and the mean of accuracies across all folds\n",
        "    \"\"\"\n",
        "    accuracies = []\n",
        "\n",
        "    for training_fold, testing_fold in round_robin_k_fold(len(documents)):\n",
        "        training_documents = documents[training_fold]\n",
        "        testing_documents = documents[testing_fold]\n",
        "        training_labels = documents_sentiments[training_fold]\n",
        "        testing_labels = documents_sentiments[testing_fold]\n",
        "\n",
        "        count_vectorizer = get_count_vectorizer(training_documents)\n",
        "        training_data = svm_process_documents(training_documents, count_vectorizer)\n",
        "\n",
        "        svm = sk.svm.LinearSVC(dual=False)\n",
        "        svm.fit(training_data, training_labels)\n",
        "\n",
        "        testing_data = svm_process_documents(testing_documents, count_vectorizer)\n",
        "        predictions = svm.predict(testing_data)\n",
        "\n",
        "        accuracy = sk.metrics.accuracy_score(predictions, testing_labels)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    return accuracies, sum(accuracies) / len(accuracies)\n",
        "\n",
        "documents_sentiments, documents, _ = read_dataset(\"reviews.json\")\n",
        "\n",
        "_, accuracy = evaluate_svm(documents, documents_sentiments)\n",
        "print(\"SVM accuracy: %0.3f\" % accuracy)\n",
        "\n",
        "_, accuracy = evaluate_naive_bayes(documents, documents_sentiments, smoothing=1)\n",
        "print(\"Naive Bayes accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "#### (Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "NOvjYe-t2Br6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.841\n"
          ]
        }
      ],
      "source": [
        "def add_pos_tags(documents: List, documents_pos_tags: List, pos_tag_filter: MutableSet = {}) -> List:\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            documents: the documents to be processed\n",
        "            documents_pos_tags: a list with the same shape as document but instead of words in contains the respective POS tags\n",
        "            pos_tag_filter: a set of POS tags of interest (if [] then no filter is applied)\n",
        "        Returns:\n",
        "            the list of documents where each word has the respective POS tag appended\n",
        "    \"\"\"\n",
        "    new_documents = []\n",
        "\n",
        "    for document, pos_tag_document in zip(documents, documents_pos_tags):\n",
        "        new_document = []\n",
        "        for sentence, pos_tag_sentence in zip(document, pos_tag_document):\n",
        "            new_sentence = []\n",
        "            for word, pos_tag in zip(sentence, pos_tag_sentence):\n",
        "                if pos_tag_filter != {}:\n",
        "                    if pos_tag in pos_tag_filter:\n",
        "                        new_word = word + pos_tag\n",
        "                        sentence.append(new_word)\n",
        "                else:\n",
        "                    new_word = word + pos_tag\n",
        "                    new_sentence.append(new_word)\n",
        "            new_document.append(new_sentence)\n",
        "        new_documents.append(new_document)\n",
        "    \n",
        "    return np.array(new_documents, dtype=object)\n",
        "\n",
        "documents_sentiments, documents, documents_pos_tags = read_dataset(\"reviews.json\")\n",
        "documents = add_pos_tags(documents, documents_pos_tags)\n",
        "\n",
        "_, accuracy = evaluate_svm(documents, documents_sentiments)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "*Write your answer here.*\n",
        "\n",
        "Accuracy: \n",
        "\n",
        "- avg SVM : 0.839\n",
        "- avg Naive Bayes: 0.824\n",
        "- avg SVM + POS tags: 0.841\n",
        "\n",
        "Including part-of-speech information helps the SVM classifier in reaching a higher cross-fold\n",
        "validation score. Using the POS tag we could tackle the word sense disambiguation.\n",
        "This helps the classifier distinguish between the sentiment in the word 'love' in the sentence \"I love this movie\" (positive) and the sentence \"This is a love story\" (neutral). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.841\n"
          ]
        }
      ],
      "source": [
        "pos_tags = {\n",
        "    \"JJ\",\n",
        "    \"JJR\",\n",
        "    \"JJS\",\n",
        "    \"RB\",\n",
        "    \"RBR\",\n",
        "    \"RBS\",\n",
        "    \"NNS\",\n",
        "    \"NN\",\n",
        "    \"VBN\",\n",
        "    \"VBG\",\n",
        "    \"VBD\",\n",
        "    \"VBP\",\n",
        "    \"VBZ\",\n",
        "    \"NNPS\",\n",
        "    \"NNP\",\n",
        "    \"VB\",\n",
        "    \"WRB\",\n",
        "    \"MD\"\n",
        "}\n",
        "\n",
        "documents_sentiments, documents, documents_pos_tags = read_dataset(\"reviews.json\")\n",
        "documents = add_pos_tags(documents, documents_pos_tags)\n",
        "\n",
        "_, accuracy = evaluate_svm(documents, documents_sentiments)\n",
        "print(\"Accuracy: %0.3f\" % accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "Accuracy: \n",
        "- avg SVM : 0.839\n",
        "- avg Naive Bayes: 0.824\n",
        "- avg SVM + POS tags: 0.841\n",
        "-  avg SVM + POS tags + removed closed-class: 0.842\n",
        "\n",
        "We expect that discarding all closed-class words from the data results in better performance.\n",
        "\n",
        "The closed classes include pronouns, determiners, conjunctions, and prepositions, whereas open-class words include nouns, lexical verbs, adjectives, and adverbs.\n",
        "\n",
        "\n",
        "The closed classes represent a more restricted range of meanings, which tend to be less detailed than open-class words.\n",
        "\n",
        "The sentiment of these closed words is something less pronounced or a bit ambiguous relative to open words which more often have a distinct sentiment. Removing ambiguity by discarding all closed-class words and using POS-tags, is expected to result in a better classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (Q4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n",
        "\n",
        "The lexicon-based approach is simpler and faster to compute but does not reach the accuracy of the other approaches. Here we only looked at the occurrence of a word to calculate a certain binary score for a given document, which yields an accuracy of 0.677 with the set threshold of 8. For the magnitude approach, the average score across the documents is calculated (thres=10.8, so the new threshold is 11) and results in a classifier accuracy of 0.696. A new method we used to define a new threshold for this approach is to incorporate document length into our calculations, which we have accomplished by dividing the score for each document by the document length and getting the average across all the documents. With this approach we obtain the new thresh 0.22, but we round it up. However, this new threshold (q1.4) does not yield a better classifier.\n",
        "\n",
        "The Naive Bayes approach reaches a far better accuracy by also taking conditional probability and prior into account as features. Using smoothing we can further increase the accuracy. To make the algorithm more robust, stemming is used to project different inflections of a word to the same feature in the bag of words vector space. Against expectation, this does not improve classifier accuracy. A negative side of stemming is that we remove some inflections of a word resulting in a smaller vocab and thus less detailed information. Using n-grams as features, we can tackle cases where our classifier only sees a positive word like good, but does not see the negation just before this word. This improves the accuracy greatly.\n",
        "\n",
        "Including POS-tags counters ambiguity, the experiments are detailed in Q3.2. Using the Support Vector Machines algorithm as our classifier results in a better accuracy score compared to the Naive Bayes (NB) approach. NB depends on conditional probabilities which are easier and faster to implement and evaluate. NB assumes that features are independent conditioned on the class of interest, but this assumption does not always hold hence the name Naive of the algorithm. SVM is more expressive in classifying non-linearly separable data, but requires an iterative process and thus is more computationally expensive. This results in a slightly higher accuracy (0.842) compared to NB (0.824). Based on these experiments it can be concluded that the Naive Bayes is the best classifier in terms of its computational cost and accuracy ratio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Student 1 #12345\n",
        "# Student 2 #12345\n",
        "\n",
        "# Erencan Tatar - 12681598\n",
        "# Andrei Blahovici - 14532921"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly. \n",
        "- Download your completed notebook using `File -> Download .ipynb` \n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a08cf5e3b06a4644fa3a0a1d848f22a5a13516db6dd76458089e9f4f837d64c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
